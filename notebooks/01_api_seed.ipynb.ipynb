{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c033d226-805a-4c1f-a74c-af10b3315266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Project Root: C:\\Users\\drrahman\\wiki-gaps-project\n",
      "✅ Config Loaded: {'project': 'wiki-gaps', 'created': '2025-10-04T22:34:57', 'language': 'en', 'seed_categories': ['Category:Living people'], 'recurse_depth': 0, 'api_sleep': 0.2, 'api_maxlag': 5, 'attrs': {'gender': 'P21', 'country': 'P27', 'occupation': 'P106'}, 'time_windows': {'start_month': '2015-01', 'end_month': None}, 'ethics': {'aggregate_only': True, 'min_cell': 20}}\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Project Setup and Configuration\n",
    "\n",
    "# This first cell imports necessary libraries and loads the project's configuration from the 'project.json' file. \n",
    "# This ensures that allsubsequent steps have access to the project's root path and settings.\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Find the project's root directory. This allows the notebook to be\n",
    "# run from the 'notebooks' subfolder without breaking file paths.\n",
    "ROOT = Path.cwd()\n",
    "if ROOT.name == \"notebooks\":\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "# Load the main configuration file.\n",
    "# This file contains all the key parameters for the project, such as\n",
    "# the starting category, API settings, and language.\n",
    "CONF_PATH = ROOT / \"conf\" / \"project.json\"\n",
    "CONF = json.load(open(CONF_PATH))\n",
    "\n",
    "# Print the root path and the loaded configuration to verify\n",
    "# that everything has been loaded correctly before proceeding.\n",
    "print(f\"✅ Project Root: {ROOT}\")\n",
    "print(f\"✅ Config Loaded: {CONF}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cea1287-f174-4994-828a-a4111eb2d05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stateless API helper function is ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: API Session and Request Handling \n",
    "\n",
    "# Uses a direct `requests.get` for each call. \n",
    "# This ensures every API request is completely independent and stateless, which is more robust against rare, state-related network issues that can occur during very long-running jobs.\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Define the English Wikipedia API endpoint\n",
    "ENWIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# Use API settings from our configuration file\n",
    "SLEEP = CONF[\"api_sleep\"]\n",
    "MAXLAG = CONF[\"api_maxlag\"]\n",
    "USER_AGENT = f\"WikiGaps/0.1 (contact: ashhik96@gmail.com)\"\n",
    "# Define headers that will be sent with every request\n",
    "HEADERS = {\"User-Agent\": USER_AGENT}\n",
    "\n",
    "def mw_get(params: dict):\n",
    "    \"\"\"\n",
    "    A stateless wrapper for making GET requests to the MediaWiki API.\n",
    "    \"\"\"\n",
    "    p = params.copy()\n",
    "    p.update({\"format\": \"json\", \"formatversion\": 2, \"maxlag\": MAXLAG})\n",
    "    \n",
    "    try:\n",
    "        # Use a simple, stateless `requests.get()` for each call\n",
    "        response = requests.get(ENWIKI_API, params=p, headers=HEADERS, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        js = response.json()\n",
    "        \n",
    "        # Check for server lag errors\n",
    "        if \"error\" in js and js[\"error\"].get(\"code\") == \"maxlag\":\n",
    "            wait_time = int(js[\"error\"].get(\"lag\", 5))\n",
    "            print(f\"Server lag detected. Waiting {wait_time}s and will skip this batch.\")\n",
    "            time.sleep(wait_time)\n",
    "            return None # Skip this batch and let the main loop continue\n",
    "\n",
    "        return js\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An API request failed: {e}\")\n",
    "        return None\n",
    "    except requests.exceptions.JSONDecodeError:\n",
    "        print(f\"Failed to decode JSON. Status: {response.status_code}, Text: {response.text[:100]}\")\n",
    "        return None\n",
    "\n",
    "print(\"✅ Stateless API helper function is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c90b50b-884f-4a77-b2ed-b8b6ace71a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Category walking functions are ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Category Walking Functions\n",
    "\n",
    "# This cell defines the functions needed to get a list of all articles\n",
    "# within a specific Wikipedia category. It's designed to handle very\n",
    "# large categories by fetching members in pages of 500 at a time.\n",
    "\n",
    "def get_category_members(category_title: str, namespace: int = 0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches all members of a single category page.\n",
    "\n",
    "    Args:\n",
    "        category_title: The full title of the category (e.g., \"Category:Living people\").\n",
    "        namespace: The namespace to search (0 for articles, 14 for subcategories).\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with the 'pageid' and 'title' of each member.\n",
    "    \"\"\"\n",
    "    member_list = []\n",
    "    continuation_token = None\n",
    "    \n",
    "    # The API returns results in pages, so we loop until the 'continue' token is gone\n",
    "    while True:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"categorymembers\",\n",
    "            \"cmtitle\": category_title,\n",
    "            \"cmnamespace\": namespace,\n",
    "            \"cmlimit\": 500,  # Request the maximum number of members per page\n",
    "        }\n",
    "        \n",
    "        # If the API gave us a continuation token, add it to the next request\n",
    "        if continuation_token:\n",
    "            params[\"cmcontinue\"] = continuation_token\n",
    "            \n",
    "        # Make the API call\n",
    "        result = mw_get(params)\n",
    "        if not result or \"query\" not in result:\n",
    "            break # Stop if the request failed or returned an empty result\n",
    "\n",
    "        # Add the retrieved members to our list\n",
    "        members = result.get(\"query\", {}).get(\"categorymembers\", [])\n",
    "        member_list.extend(members)\n",
    "        \n",
    "        # Check for a new continuation token to get the next page\n",
    "        continuation_token = result.get(\"continue\", {}).get(\"cmcontinue\")\n",
    "        if not continuation_token:\n",
    "            break # No more pages, so we're done\n",
    "            \n",
    "        time.sleep(SLEEP) # Be polite and pause between requests\n",
    "        \n",
    "    if not member_list:\n",
    "        return pd.DataFrame(columns=[\"pageid\", \"title\"])\n",
    "        \n",
    "    # Convert the list of results into a clean DataFrame\n",
    "    return pd.DataFrame(member_list)[[\"pageid\", \"title\"]].drop_duplicates()\n",
    "\n",
    "print(\"✅ Category walking functions are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e0c928-2e32-43c8-ac35-f1b96a70e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to walk through 1 seed categor(y/ies)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21807a4e0b34e018027f880e1a26984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Categories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching members for: Category:Living people...\n",
      "\n",
      "✅ Found a total of 1,128,338 unique pages.\n",
      "Sample of the seed pages DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageid</th>\n",
       "      <th>title</th>\n",
       "      <th>seed_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>340</td>\n",
       "      <td>Alain Connes</td>\n",
       "      <td>Category:Living people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>595</td>\n",
       "      <td>Andre Agassi</td>\n",
       "      <td>Category:Living people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>Anna Kournikova</td>\n",
       "      <td>Category:Living people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>910</td>\n",
       "      <td>Arne Kaijser</td>\n",
       "      <td>Category:Living people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1020</td>\n",
       "      <td>Anatoly Karpov</td>\n",
       "      <td>Category:Living people</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pageid            title           seed_category\n",
       "0     340     Alain Connes  Category:Living people\n",
       "1     595     Andre Agassi  Category:Living people\n",
       "2     890  Anna Kournikova  Category:Living people\n",
       "3     910     Arne Kaijser  Category:Living people\n",
       "4    1020   Anatoly Karpov  Category:Living people"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 4: Execute the Category Walk\n",
    "\n",
    "# This cell runs the main process to enumerate all articles in the seed categories.\n",
    "# It uses the 'get_category_members' function from the previous cell and a\n",
    "# progress bar to track the process for each starting category.\n",
    "\n",
    "all_pages_frames = []\n",
    "seed_categories = CONF[\"seed_categories\"]\n",
    "\n",
    "print(f\"Starting to walk through {len(seed_categories)} seed categor(y/ies)...\")\n",
    "\n",
    "# Loop through each category defined in the project.json configuration\n",
    "for category in tqdm(seed_categories, desc=\"Processing Categories\"):\n",
    "    print(f\"Fetching members for: {category}...\")\n",
    "    \n",
    "    # Fetch all the article pages (namespace=0) in the category\n",
    "    pages_df = get_category_members(category, namespace=0)\n",
    "    \n",
    "    # Add a column to track which seed category this page came from\n",
    "    if not pages_df.empty:\n",
    "        pages_df[\"seed_category\"] = category\n",
    "        all_pages_frames.append(pages_df)\n",
    "\n",
    "# Combine the results from all categories into a single DataFrame\n",
    "if all_pages_frames:\n",
    "    seed_pages_df = pd.concat(all_pages_frames, ignore_index=True)\n",
    "\n",
    "    # Clean the final DataFrame by removing any duplicate pages (if categories overlap),\n",
    "    # sorting by pageid, and resetting the index for a clean output.\n",
    "    seed_pages_df = (\n",
    "        seed_pages_df\n",
    "        .drop_duplicates(subset=[\"pageid\"])\n",
    "        .sort_values(\"pageid\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Display the total number of pages found and a sample of the data\n",
    "    print(f\"\\n✅ Found a total of {len(seed_pages_df):,} unique pages.\")\n",
    "    print(\"Sample of the seed pages DataFrame:\")\n",
    "    display(seed_pages_df.head())\n",
    "else:\n",
    "    print(\"\\n⚠️ No pages found. Check your seed categories in project.json.\")\n",
    "    # Create an empty DataFrame to prevent errors in later cells\n",
    "    seed_pages_df = pd.DataFrame(columns=[\"pageid\", \"title\", \"seed_category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be03a8cc-ee8a-4411-bec4-e82aaff5c1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Corrected Page ID to QID mapping function is ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Page ID to Wikidata QID Mapping Function \n",
    "\n",
    "import math\n",
    "\n",
    "def map_pageids_to_qids(pages_df: pd.DataFrame, batch_size: int = 50) -> pd.DataFrame:\n",
    "    pageids = pages_df[\"pageid\"].tolist()\n",
    "    all_mapped_pages = []\n",
    "\n",
    "    batch_range = range(0, len(pageids), batch_size)\n",
    "    for i in tqdm(batch_range, desc=\"Mapping Page IDs to QIDs\"):\n",
    "        id_batch = pageids[i:i + batch_size]\n",
    "        id_string = \"|\".join(map(str, id_batch))\n",
    "        \n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"prop\": \"pageprops\",\n",
    "            \"ppprop\": \"wikibase_item\",\n",
    "            \"pageids\": id_string,\n",
    "            \"redirects\": 1,\n",
    "        }\n",
    "        \n",
    "        result = mw_get(params)\n",
    "        \n",
    "        # --- THIS IS THE CORRECTED LOGIC ---\n",
    "        # It now correctly checks for 'pages' inside the 'query' dictionary.\n",
    "        if result and \"query\" in result and \"pages\" in result.get(\"query\", {}):\n",
    "            for page_info in result[\"query\"][\"pages\"]:\n",
    "                qid = page_info.get(\"pageprops\", {}).get(\"wikibase_item\")\n",
    "                if qid:\n",
    "                    all_mapped_pages.append({\n",
    "                        \"pageid\": page_info.get(\"pageid\"),\n",
    "                        \"title\": page_info.get(\"title\"),\n",
    "                        \"qid\": qid\n",
    "                    })\n",
    "        \n",
    "        time.sleep(SLEEP)\n",
    "\n",
    "    # Handle the case where no QIDs were found at all\n",
    "    if not all_mapped_pages:\n",
    "        return pd.DataFrame(columns=['pageid', 'title', 'qid'])\n",
    "\n",
    "    return pd.DataFrame(all_mapped_pages)\n",
    "\n",
    "print(\"✅ Corrected Page ID to QID mapping function is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd68afb8-c04f-4eed-a24c-9f2792181159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting a small-scale test on 500 pages ---\n",
      "Sample size: 500 pages.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d36a6da3a546c9b4d79f0278a5da42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mapping Page IDs to QIDs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ TEST SUCCESSFUL: Mapped 500 pages to unique QIDs.\n",
      "Sample of the test results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageid</th>\n",
       "      <th>title</th>\n",
       "      <th>qid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>340</td>\n",
       "      <td>Alain Connes</td>\n",
       "      <td>Q313590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>595</td>\n",
       "      <td>Andre Agassi</td>\n",
       "      <td>Q7407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>Anna Kournikova</td>\n",
       "      <td>Q131120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>910</td>\n",
       "      <td>Arne Kaijser</td>\n",
       "      <td>Q4794599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1020</td>\n",
       "      <td>Anatoly Karpov</td>\n",
       "      <td>Q131674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pageid            title       qid\n",
       "0     340     Alain Connes   Q313590\n",
       "1     595     Andre Agassi     Q7407\n",
       "2     890  Anna Kournikova   Q131120\n",
       "3     910     Arne Kaijser  Q4794599\n",
       "4    1020   Anatoly Karpov   Q131674"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 6A: Small-Scale Test Run\n",
    "\n",
    "# Before running the full multi-hour process, this cell tests the entire mapping and cleaning pipeline on a small sample of 500 pages.\n",
    "# If this cell completes successfully, we can be confident the full run will work.\n",
    "\n",
    "print(\"--- Starting a small-scale test on 500 pages ---\")\n",
    "\n",
    "# Create a small sample from our main DataFrame\n",
    "sample_df = seed_pages_df.head(500)\n",
    "print(f\"Sample size: {len(sample_df)} pages.\")\n",
    "\n",
    "# Run the same mapping function on the smaller sample\n",
    "test_qids_df = map_pageids_to_qids(sample_df)\n",
    "\n",
    "# Use the same robust checking and cleaning logic as the main cell\n",
    "if not test_qids_df.empty and 'qid' in test_qids_df.columns:\n",
    "    test_qids_df_unique = (\n",
    "        test_qids_df\n",
    "        .dropna(subset=[\"qid\"])\n",
    "        .drop_duplicates(subset=[\"qid\"])\n",
    "        .sort_values(\"pageid\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    print(f\"\\n✅ TEST SUCCESSFUL: Mapped {len(test_qids_df_unique)} pages to unique QIDs.\")\n",
    "    print(\"Sample of the test results:\")\n",
    "    display(test_qids_df_unique.head())\n",
    "else:\n",
    "    print(\"\\n⚠️ TEST FAILED: The mapping process returned no data even for a small sample.\")\n",
    "    print(\"There may still be an underlying network or API issue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37007476-85ab-4666-9409-e5fb2dc375f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the mapping process. This will take a long time...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c052e7f37b742a9ae76f494bb7478ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mapping Page IDs to QIDs:   0%|          | 0/22567 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Successfully mapped 1,125,607 pages to unique QIDs.\n",
      "Sample of the final QID DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageid</th>\n",
       "      <th>title</th>\n",
       "      <th>qid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>340</td>\n",
       "      <td>Alain Connes</td>\n",
       "      <td>Q313590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>595</td>\n",
       "      <td>Andre Agassi</td>\n",
       "      <td>Q7407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>Anna Kournikova</td>\n",
       "      <td>Q131120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>910</td>\n",
       "      <td>Arne Kaijser</td>\n",
       "      <td>Q4794599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1020</td>\n",
       "      <td>Anatoly Karpov</td>\n",
       "      <td>Q131674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pageid            title       qid\n",
       "0     340     Alain Connes   Q313590\n",
       "1     595     Andre Agassi     Q7407\n",
       "2     890  Anna Kournikova   Q131120\n",
       "3     910     Arne Kaijser  Q4794599\n",
       "4    1020   Anatoly Karpov   Q131674"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 6B: Execute the Page ID to QID Mapping \n",
    "\n",
    "# This cell calls the mapping function from the previous step to fetch the Wikidata QID for every page. \n",
    "# Includes a check to ensure data was actually collected before attempting to clean it.\n",
    "\n",
    "print(\"Starting the mapping process. This will take a long time...\")\n",
    "\n",
    "qids_df = map_pageids_to_qids(seed_pages_df)\n",
    "\n",
    "# Check if the process returned a DataFrame with a 'qid' column before processing\n",
    "if not qids_df.empty and 'qid' in qids_df.columns:\n",
    "    # It's possible for multiple pages (e.g., redirects) to map to the same QID.\n",
    "    # We'll clean the final list by dropping any duplicate QIDs to ensure each\n",
    "    # person is represented only once.\n",
    "    qids_df_unique = (\n",
    "        qids_df\n",
    "        .dropna(subset=[\"qid\"])\n",
    "        .drop_duplicates(subset=[\"qid\"])\n",
    "        .sort_values(\"pageid\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Display the total number of unique QIDs found and a sample of the data\n",
    "    print(f\"\\n✅ Successfully mapped {len(qids_df_unique):,} pages to unique QIDs.\")\n",
    "    print(\"Sample of the final QID DataFrame:\")\n",
    "    display(qids_df_unique.head())\n",
    "\n",
    "else:\n",
    "    print(\"\\n⚠️ Error: The mapping process completed but returned no data.\")\n",
    "    print(\"This might be due to a network issue or a problem with the API.\")\n",
    "    print(\"Please check your internet connection and consider re-running this cell.\")\n",
    "    # Create an empty DataFrame with the correct columns to prevent future errors\n",
    "    qids_df_unique = pd.DataFrame(columns=['pageid', 'title', 'qid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00bb8a8-9733-4bf9-bf5d-9eeb60cfd2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ (one-by-one) timestamp function is ready.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7A: Fetch Creation Timestamps Function \n",
    "\n",
    "# The Wikipedia API requires us to ask for the first revision of each page individually, rather than in batches.\n",
    "# This function loops through each pageid and makes a separate request.\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_creation_timestamps(pages_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches the creation timestamp for a list of pageids, one at a time.\n",
    "    \"\"\"\n",
    "    pageids = pages_df[\"pageid\"].tolist()\n",
    "    timestamps = []\n",
    "\n",
    "    # Loop through each pageid individually\n",
    "    for pageid in tqdm(pageids, desc=\"Fetching Creation Timestamps\"):\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"prop\": \"revisions\",\n",
    "            \"rvprop\": \"timestamp\",\n",
    "            \"rvlimit\": 1,\n",
    "            \"rvdir\": \"newer\",\n",
    "            \"pageids\": pageid, # Send only one pageid at a time\n",
    "        }\n",
    "        \n",
    "        result = mw_get(params)\n",
    "        \n",
    "        if result and \"query\" in result and \"pages\" in result.get(\"query\", {}):\n",
    "            # The response will contain only one page_info object\n",
    "            page_info = result[\"query\"][\"pages\"][0]\n",
    "            timestamp = page_info.get(\"revisions\", [{}])[0].get(\"timestamp\")\n",
    "            if timestamp:\n",
    "                timestamps.append({\n",
    "                    \"pageid\": page_info.get(\"pageid\"),\n",
    "                    \"creation_timestamp\": timestamp\n",
    "                })\n",
    "        \n",
    "        # A very short sleep is sufficient here\n",
    "        time.sleep(0.02)\n",
    "\n",
    "    if not timestamps:\n",
    "        return pd.DataFrame(columns=['pageid', 'creation_timestamp'])\n",
    "\n",
    "    return pd.DataFrame(timestamps)\n",
    "\n",
    "print(\"✅ (one-by-one) timestamp function is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b32a47fe-1a2c-4e76-94e1-a12582a920b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting a small-scale test for timestamps on 500 pages ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e124fc357f0a4e10b48180a274ced829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching Creation Timestamps:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ TIMESTAMP TEST SUCCESSFUL.\n",
      "Sample of the test results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageid</th>\n",
       "      <th>creation_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>340</td>\n",
       "      <td>2001-09-08T15:21:56Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>595</td>\n",
       "      <td>2001-02-06T20:50:01Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>2001-08-28T13:25:02Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>910</td>\n",
       "      <td>2001-05-19T15:58:12Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1020</td>\n",
       "      <td>2001-06-15T16:43:42Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pageid    creation_timestamp\n",
       "0     340  2001-09-08T15:21:56Z\n",
       "1     595  2001-02-06T20:50:01Z\n",
       "2     890  2001-08-28T13:25:02Z\n",
       "3     910  2001-05-19T15:58:12Z\n",
       "4    1020  2001-06-15T16:43:42Z"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 7B: Small-Scale Test for Timestamps\n",
    "\n",
    "# Fetching process on a small sample before starting the full run.\n",
    "\n",
    "print(\"--- Starting a small-scale test for timestamps on 500 pages ---\")\n",
    "\n",
    "# Use the 'qids_df_unique' DataFrame that was created successfully in Cell 6\n",
    "sample_df = qids_df_unique.head(500)\n",
    "\n",
    "test_timestamps_df = get_creation_timestamps(sample_df)\n",
    "\n",
    "if not test_timestamps_df.empty:\n",
    "    print(\"\\n✅ TIMESTAMP TEST SUCCESSFUL.\")\n",
    "    print(\"Sample of the test results:\")\n",
    "    display(test_timestamps_df.head())\n",
    "else:\n",
    "    print(\"\\n⚠️ TIMESTAMP TEST FAILED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a67b9b06-0797-49ef-b10c-58dcdf2e47c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to fetch creation timestamps...\n",
      "Resuming from existing file: timestamps_partial.csv\n",
      "Loaded 940,000 existing timestamps. Resuming...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2455abfdcc48b3bb87b339f79778da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching Creation Timestamps:   0%|          | 0/185702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved progress: 950,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 960,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 970,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 980,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 990,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 1,000,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 1,010,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 1,020,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 1,030,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 1,040,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 1,050,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 1,060,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 1,070,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 1,080,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 1,090,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 1,100,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 1,110,000 total timestamps collected.\n",
      "\n",
      "Saved progress: 1,120,000 total timestamps collected.\n",
      "\n",
      "✅ Successfully fetched all timestamps for 1,125,694 pages.\n",
      "Sample of the final timestamps DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageid</th>\n",
       "      <th>creation_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>340</td>\n",
       "      <td>2001-09-08T15:21:56Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>595</td>\n",
       "      <td>2001-02-06T20:50:01Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>2001-08-28T13:25:02Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>910</td>\n",
       "      <td>2001-05-19T15:58:12Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1020</td>\n",
       "      <td>2001-06-15T16:43:42Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pageid    creation_timestamp\n",
       "0     340  2001-09-08T15:21:56Z\n",
       "1     595  2001-02-06T20:50:01Z\n",
       "2     890  2001-08-28T13:25:02Z\n",
       "3     910  2001-05-19T15:58:12Z\n",
       "4    1020  2001-06-15T16:43:42Z"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 8: Execute Timestamp Fetching (with Incremental Saves)\n",
    "\n",
    "# This version saves progress to a CSV file after every 10,000 pages.\n",
    "# This means you can safely stop the script at any time and it will automatically resume where it left off the next time you run it.\n",
    "\n",
    "print(\"Starting to fetch creation timestamps...\")\n",
    "\n",
    "# Define the output path and check for existing data to resume from\n",
    "output_path = ROOT / \"data\" / \"processed\" / \"timestamps_partial.csv\"\n",
    "timestamps_list = []\n",
    "processed_pageids = set()\n",
    "\n",
    "if output_path.exists():\n",
    "    print(f\"Resuming from existing file: {output_path.name}\")\n",
    "    existing_df = pd.read_csv(output_path)\n",
    "    timestamps_list = existing_df.to_dict('records')\n",
    "    processed_pageids = set(existing_df['pageid'])\n",
    "    print(f\"Loaded {len(processed_pageids):,} existing timestamps. Resuming...\")\n",
    "\n",
    "# Filter out pages we already have timestamps for\n",
    "pages_to_fetch_df = qids_df_unique[~qids_df_unique['pageid'].isin(processed_pageids)]\n",
    "\n",
    "if pages_to_fetch_df.empty:\n",
    "    print(\"All timestamps have already been fetched.\")\n",
    "    timestamps_df = pd.DataFrame(timestamps_list)\n",
    "else:\n",
    "    # Loop through each pageid that still needs to be fetched\n",
    "    for pageid in tqdm(pages_to_fetch_df['pageid'].tolist(), desc=\"Fetching Creation Timestamps\"):\n",
    "        params = {\n",
    "            \"action\": \"query\", \"prop\": \"revisions\", \"rvprop\": \"timestamp\",\n",
    "            \"rvlimit\": 1, \"rvdir\": \"newer\", \"pageids\": pageid,\n",
    "        }\n",
    "        \n",
    "        result = mw_get(params)\n",
    "        \n",
    "        if result and \"query\" in result and \"pages\" in result.get(\"query\", {}):\n",
    "            page_info = result[\"query\"][\"pages\"][0]\n",
    "            timestamp = page_info.get(\"revisions\", [{}])[0].get(\"timestamp\")\n",
    "            if timestamp:\n",
    "                timestamps_list.append({\n",
    "                    \"pageid\": page_info.get(\"pageid\"),\n",
    "                    \"creation_timestamp\": timestamp\n",
    "                })\n",
    "\n",
    "        # --- Incremental Save Logic ---\n",
    "        # Save after every 10,000 new items are collected\n",
    "        if len(timestamps_list) > 0 and len(timestamps_list) % 10000 == 0:\n",
    "             if len(timestamps_list) > len(processed_pageids):\n",
    "                pd.DataFrame(timestamps_list).to_csv(output_path, index=False)\n",
    "                print(f\"\\nSaved progress: {len(timestamps_list):,} total timestamps collected.\")\n",
    "        \n",
    "        time.sleep(0.02)\n",
    "\n",
    "# Final save at the end\n",
    "timestamps_df = pd.DataFrame(timestamps_list)\n",
    "if not timestamps_df.empty:\n",
    "    timestamps_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Successfully fetched all timestamps for {len(timestamps_df):,} pages.\")\n",
    "print(\"Sample of the final timestamps DataFrame:\")\n",
    "display(timestamps_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d3cc72c-bf79-4af0-a231-84144fb1a65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging QIDs and timestamps...\n",
      "\n",
      "✅ Success! Notebook 01 is complete.\n",
      "Final dataset saved to: seed_enwiki_20251007-213232.csv\n",
      "Total rows: 1,125,607\n",
      "Sample of the final output:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageid</th>\n",
       "      <th>title</th>\n",
       "      <th>qid</th>\n",
       "      <th>first_edit_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>340</td>\n",
       "      <td>Alain Connes</td>\n",
       "      <td>Q313590</td>\n",
       "      <td>2001-09-08T15:21:56Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>595</td>\n",
       "      <td>Andre Agassi</td>\n",
       "      <td>Q7407</td>\n",
       "      <td>2001-02-06T20:50:01Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>Anna Kournikova</td>\n",
       "      <td>Q131120</td>\n",
       "      <td>2001-08-28T13:25:02Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>910</td>\n",
       "      <td>Arne Kaijser</td>\n",
       "      <td>Q4794599</td>\n",
       "      <td>2001-05-19T15:58:12Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1020</td>\n",
       "      <td>Anatoly Karpov</td>\n",
       "      <td>Q131674</td>\n",
       "      <td>2001-06-15T16:43:42Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pageid            title       qid         first_edit_ts\n",
       "0     340     Alain Connes   Q313590  2001-09-08T15:21:56Z\n",
       "1     595     Andre Agassi     Q7407  2001-02-06T20:50:01Z\n",
       "2     890  Anna Kournikova   Q131120  2001-08-28T13:25:02Z\n",
       "3     910     Arne Kaijser  Q4794599  2001-05-19T15:58:12Z\n",
       "4    1020   Anatoly Karpov   Q131674  2001-06-15T16:43:42Z"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 9: Merge Data and Save Final Output\n",
    "\n",
    "# Merge the DataFrame containing the QIDs with the DataFrame containing the creation timestamps and save the result to a single CSV file in the 'data/raw' directory.\n",
    "\n",
    "print(\"Merging QIDs and timestamps...\")\n",
    "\n",
    "# Merge the two DataFrames on the 'pageid' column.\n",
    "# 'Left' merge to ensure all pages from our main QID list.\n",
    "final_df = pd.merge(qids_df_unique, timestamps_df, on=\"pageid\", how=\"left\")\n",
    "\n",
    "# Rename the 'creation_timestamp' column to 'first_edit_ts' to match the project schema.\n",
    "final_df = final_df.rename(columns={\"creation_timestamp\": \"first_edit_ts\"})\n",
    "\n",
    "# Select and reorder the columns for the final output file.\n",
    "output_columns = [\"pageid\", \"title\", \"qid\", \"first_edit_ts\"]\n",
    "final_df = final_df[output_columns]\n",
    "\n",
    "# Generate a timestamped filename for the output file.\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "output_path = ROOT / \"data\" / \"raw\" / f\"seed_enwiki_{ts}.csv\"\n",
    "\n",
    "# Save the final DataFrame to a CSV file.\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Success! Notebook 01 is complete.\")\n",
    "print(f\"Final dataset saved to: {output_path.name}\")\n",
    "print(f\"Total rows: {len(final_df):,}\")\n",
    "print(\"Sample of the final output:\")\n",
    "display(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8373327-721c-4273-8dce-06e02085da00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
